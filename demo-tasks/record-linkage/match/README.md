## Instructions

Update all tasks by running `make`. Review and address any errors if they
occur.

## Specific exercises

### `blocking` and `blocking-features`

1. `TS-import` brings in labeled training data, that is currently not used
   anywhere in the repo. Incorporate the labeled data into the blocking
   pipeline by writing a script that reports how many of the known true matches
   are covered by `blocking/output/candidate-pairs.parquet`.
2. The reason we separate out the logic to count the number of pairs for a rule
   is because we often want to explore many possible blocking rules to see
   which would be the "best" in terms of both covering known matches (see
   exercise 1) and not generating too many pairs overall. Since we can
   calculate the number of pairs that would be generated much faster than we
   can produce the pairs, it helps to have a separate script that just does
   that. Use the count-pairs script, and your answer to exercise 1, to explore
   and compare different rules. Once you feel confident, try to set up a search
   like the one described in [Database Deduplication to Identify Victims of
   Human Rights
   Violations](https://hrdag.org/2016/01/08/a-geeky-deep-dive-database-deduplication-to-identify-victims-of-human-rights-violations/)
3. A search like the one in step 2 is bottlenecked by how long it takes to
   calculate the number of pairs generated by a given rule. Write a
   benchmarking script that measures the amount of time required to count the
   generated pairs for one or more rules, and then try to write your  own
   count-pairs script that is more performant that the one provided.

## `compare` & `classify`

1. if data is big, loading the full pairs data and doing all of our string
   distance calculations might take too much memory/time. Modify the compare
   script to calculate features one at a time, and then add a final step that
   re-combines all of the feature-ized pair data.
2. similarly, given large data sets we can make better use of our computation
   resources by running `classify` on chunks of records, rather than the entire
   data set all at once. Update classify to run on chunks of 10,000 pairs at a
   time.
3. currently `classify` is based on some adhoc heuristics. Evaluate the quality
   of the solution by comparing to the known labels from `TS-import`. Then
   train a machine learning classifier to classify pairs. Does this work better
   than the heuristics? Feel free to create new/different features in
   `compare`, your solution to exercise 1 should allow you to add more
   features without running into memory limitations.

## `cluster`

1. Take a look at the diagnostics presented in
   `cluster/output/cluster-summary.yaml`. `degree_distribution` summarizes the
   distribution of number of records that a given record is linked to.
   `recs_per_entity` summarizes the distribution of unique records associated
   with each entity. What do you see there that would concern you about the
   match quality. Encode your expectations as a test. What other metrics would
   you include in your test?

2. Create a version of `cluster` that can pass your expectation tests, i.e.
   that does not result in any improbably large clusters. Hint: take a look at
   [Clustering and solving the right
   problem](https://hrdag.org/2016/07/28/clustering-and-solving-the-right-problem/)


## Further reading

[string metric wikipedia entry](https://en.wikipedia.org/wiki/String_metric)

To-do: other readings

